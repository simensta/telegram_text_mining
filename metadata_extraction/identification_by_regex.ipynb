{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import unittest\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Metadata with Regex\n",
    "This script leverages regular experessions (regex) to identify metadata within the Thomas T. Eckert telegram collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "doc_pattern = r'.*/preprocessed_.*.txt'\n",
    "category_pattern = r'.*?/(\\w+_telegrams)/'\n",
    "path_to_corpus = '/Volumes/data_work/dcw_text_mining/eckert_papers_corpus/'\n",
    "telegram_corpus = CategorizedPlaintextCorpusReader(\n",
    "    path_to_corpus,\n",
    "    doc_pattern,\n",
    "    cat_pattern=category_pattern\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata Resources\n",
    "First, the script will load resources we already have for identifying metadata. In this case, a csv of known people and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_people_file = \"/Volumes/data_work/dcw_text_mining/metadata_id_resources/dcw_names.csv\"\n",
    "path_to_locations_file = \"/Volumes/data_work/dcw_text_mining/metadata_id_resources/dcw_locations.csv\"\n",
    "path_to_state_abrrev_file = \"/Volumes/data_work/dcw_text_mining/metadata_id_resources/postal_abbreviations_for_states_territories.csv\"\n",
    "path_to_title_file = \"/Volumes/data_work/dcw_text_mining/metadata_id_resources/titles.csv\"\n",
    "\n",
    "people = pd.read_csv(path_to_people_file)\n",
    "\n",
    "# The surnmames field is slightly normalized, e.g., replace Ã© with e\n",
    "surnames = people['Surnames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv(path_to_title_file)\n",
    "abbrevs = titles.abbreviation.to_numpy()\n",
    "full_titles = [title.lower() for title in titles.definition.to_numpy()]\n",
    "all_titles = np.append(abbrevs, full_titles)\n",
    "title_soup = [title.split() for title in all_titles]\n",
    "title_tokens = [num for elem in title_soup for num in elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv(path_to_locations_file)\n",
    "location_terms = locations['Term']\n",
    "\n",
    "# https://about.usps.com/who-we-are/postal-history/state-abbreviations.pdf\n",
    "state_abbreviations = pd.read_csv(path_to_state_abrrev_file)\n",
    "states = []\n",
    "for state in state_abbreviations[\"State\"]:\n",
    "    states.append(state.lower())\n",
    "\n",
    "for state in state_abbreviations[\"1831\"]:\n",
    "    states.append(state.lower())\n",
    "    \n",
    "for state in state_abbreviations[\"1874\"]:\n",
    "    states.append(state.lower())\n",
    "\n",
    "state_soup = set(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ordinals = ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'eleventh', 'twelfth', 'thirteenth', 'fourteenth', 'fifteenth', 'sixteenth', 'seventeenth', 'eighteenth', 'nineteenth', 'tenth', 'twentieth', 'thirtieth', 'fortieth', 'fiftieth', 'sixtieth', 'seventieth', 'eightieth', 'ninetieth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_time = r\"\\d{1,2}\\W{0,1}\\d{0,2} (am|a m)\"\n",
    "strict_meridiem = r\"\\b12\\W{0,1}\\b (\\bm\\b|\\bmid\\b)\"\n",
    "full_date_pattern = re.compile(r\"\"\"\n",
    "    (\\b\n",
    "        (jan|jany|jany|feby|feb|march|mch|mar|april|apl|apr|may|june|july|jul|august|aug|september|sept|october|oct|nov|dec)\n",
    "    \\b)\n",
    "    \\s{,1} # up to one whitespace deliminator\n",
    "    (?P<day>\\d{,2}\n",
    "        (?P<day_suffix>st|d|th|nd|rd)\n",
    "    *)\n",
    "    \\s{,1} # up to one whitespace deliminator\n",
    "    (?P<year>\\b(18){0,1}([5-7][0-9]){0,1}\\b)\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "salutations = ['obt servt', 'signed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for finding regular expression matches\n",
    "The following methods identify patterns within a telegram based on the supplied regular expressions. Matches (i.e., metadata extractions) are stored in a telegram objects until the match can be exported to a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_telegram_object():\n",
    "    temp_telegram_object = {}\n",
    "    temp_telegram_object[\"Dates\"] = []\n",
    "    temp_telegram_object[\"Times\"] = []\n",
    "    temp_telegram_object[\"People\"] = []\n",
    "    temp_telegram_object[\"Locations\"] = []\n",
    "    return temp_telegram_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicates_by_location(key, temp_telegram_object, start, end):\n",
    "    if len(temp_telegram_object[key]) == 0:\n",
    "        return\n",
    "    for element in temp_telegram_object[key]:\n",
    "        matching_indices = (int(start) == int(element['start']) and int(end) == int(element['end']))\n",
    "        if matching_indices:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_word(word_end_index, line):\n",
    "    empty_token_list = len(line[word_end_index:-1].split()) == 0\n",
    "    last_index_math = word_end_index == (len(line) - 1)\n",
    "    return (empty_token_list or last_index_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token(line, end_prev):\n",
    "    last_index = len(line) - 1\n",
    "    if last_index == end_prev:\n",
    "        return None\n",
    "    beginning_of_next_token = end_prev + 1\n",
    "    if last_index == beginning_of_next_token:\n",
    "        return None\n",
    "    rest_of_telegram = line[beginning_of_next_token:].split()\n",
    "    if len(rest_of_telegram) == 0:\n",
    "        return None\n",
    "    next_token = line[beginning_of_next_token:].split()[0]\n",
    "    len_next_token = len(next_token)\n",
    "    end_of_next_token = beginning_of_next_token + len_next_token\n",
    "    return (next_token, beginning_of_next_token, end_of_next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_token(line, start_proceeding):\n",
    "    end_of_previous_token = start_proceeding - 1\n",
    "    previous_token = line[:end_of_proceeding_token].split()[0]\n",
    "    len_previous_token = len(previous_token)\n",
    "    beginning_of_previous_token = previous_token - len_previous_token\n",
    "    return (previous_token, beginning_of_previous_token, end_of_previous_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for finding candidates for people, sender, and recipient tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_people_in_temp_telegram_object(temp_telegram_object, data_type, text, controlled, lc_number, start, recd, end):\n",
    "    if not check_for_duplicates_by_location(\"People\", temp_telegram_object, start, end):\n",
    "        temp_telegram_object[\"People\"].append({\n",
    "            \"data_type\": data_type,\n",
    "            \"text\": text,\n",
    "            \"controlled\": controlled,\n",
    "            \"lc_number\": lc_number,\n",
    "            \"start\": start,\n",
    "            \"recd\": recd,\n",
    "            \"end\": end\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pos_tag(line, token_end):\n",
    "    token_index = len(line[0:token_end].split())\n",
    "    pos_token = nltk.pos_tag(line.split())[token_index - 1]\n",
    "    if pos_token[1] == 'JJ':\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_or_add_person(line, temp_telegram_object, start, end, token):\n",
    "    captured = False\n",
    "    for element in temp_telegram_object[\"People\"]:\n",
    "        if int(start) == int(element['start']) and int(end) == int(element['end']):\n",
    "            element['data_type'] = 'Sender'\n",
    "            captured = True\n",
    "            break\n",
    "    if not captured:\n",
    "        if check_pos_tag(line, end):\n",
    "            update_people_in_temp_telegram_object(\n",
    "                temp_telegram_object,\n",
    "                'Sender',\n",
    "                token,\n",
    "                '',\n",
    "                '',\n",
    "                start,\n",
    "                False,\n",
    "                end\n",
    "            )\n",
    "    return temp_telegram_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: filter initials\n",
    "def find_surname_candidate(canidate, beginning_index, end_index, line):\n",
    "    if len(canidate) > 2:\n",
    "        return (canidate, beginning_index, end_index)\n",
    "    else:\n",
    "        next_canidate = next_token(line, end_index)\n",
    "        if next_canidate is None:\n",
    "            return None\n",
    "        return find_surname_candidate(next_canidate[0], next_canidate[1], next_canidate[2], line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_titles(canidate, beginning_index, end_index, line):\n",
    "    if canidate not in title_tokens:\n",
    "        return (canidate, beginning_index, end_index)\n",
    "    else:\n",
    "        canidate = next_token(line, end_index)\n",
    "        return find_titles(canidate[0], canidate[1], canidate[2], line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_person_by_title(line, temp_telegram_object):\n",
    "    # using known titles search for individuals\n",
    "    for title in all_titles:\n",
    "        title_pattern = re.compile(r\"\\b{}\\b\".format(title))\n",
    "        \n",
    "        for match in re.finditer(title_pattern, line):\n",
    "            data_type = \"People\"\n",
    "            recd_status = False\n",
    "            lc_number = \"\"\n",
    "            \n",
    "            # is the title the last word in the telegram?\n",
    "            if last_word(match.end(), line):\n",
    "                return\n",
    "    \n",
    "            # stores a tuple of (next_token, beginning index, end index)\n",
    "            surname_canidate = next_token(line, match.end())\n",
    "            \n",
    "            # is the word after the title the last word in the telegram?\n",
    "            if last_word(surname_canidate[2], line):\n",
    "                return\n",
    "            \n",
    "            # check if the 'word' after the title is an intial\n",
    "            # if None is returned, the script has likely encountered the end of the telegram \n",
    "            surname_canidate = find_surname_candidate(surname_canidate[0], surname_canidate[1], surname_canidate[2], line)\n",
    "            if surname_canidate is None:\n",
    "                continue\n",
    "            # do any of the token match known surnames\n",
    "            surname_df = people[people.Surnames ==  surname_canidate[0].title()]\n",
    "            \n",
    "            # check number of surname matches\n",
    "            if len(surname_df) > 0:\n",
    "                if len(surname_df) > 1:\n",
    "                    if check_pos_tag(line, surname_canidate[2]):\n",
    "                        update_people_in_temp_telegram_object(\n",
    "                            temp_telegram_object,\n",
    "                            data_type,\n",
    "                            surname_canidate[0],\n",
    "                            '',\n",
    "                            'multi-match',\n",
    "                            surname_canidate[1],\n",
    "                            recd_status,\n",
    "                            surname_canidate[2]\n",
    "                        )\n",
    "                else:\n",
    "                    # only 1 matching known surname\n",
    "                    lc_number = surname_df['LC_number'].values[0]\n",
    "                    if check_pos_tag(line, surname_canidate[2]):\n",
    "                        update_people_in_temp_telegram_object(\n",
    "                            temp_telegram_object,\n",
    "                            data_type,\n",
    "                            surname_canidate[0],\n",
    "                            '',\n",
    "                            lc_number,\n",
    "                            surname_canidate[1],\n",
    "                            recd_status,\n",
    "                            surname_canidate[2]\n",
    "                        )\n",
    "                    \n",
    "            # if ends with an s, remove s and check    \n",
    "            else:\n",
    "                if surname_canidate[0][-1] is 's':\n",
    "                    post_token_wo_s = surname_canidate[0][0:-1]\n",
    "                    surname_wo_s_df = people[people.Surnames == post_token_wo_s.title()]\n",
    "                    if len(surname_wo_s_df) > 0:\n",
    "                        if check_pos_tag(line, surname_canidate[2]):\n",
    "                            update_people_in_temp_telegram_object(\n",
    "                                temp_telegram_object,\n",
    "                                data_type,\n",
    "                                post_token_wo_s,\n",
    "                                '',\n",
    "                                lc_number,\n",
    "                                surname_canidate[1],\n",
    "                                recd_status,\n",
    "                                surname_canidate[2] - 1\n",
    "                            )\n",
    "                    else:\n",
    "                        if check_pos_tag(line, surname_canidate[2]):\n",
    "                            update_people_in_temp_telegram_object(\n",
    "                                temp_telegram_object,\n",
    "                                data_type,\n",
    "                                surname_canidate[0],\n",
    "                                '',\n",
    "                                lc_number,\n",
    "                                surname_canidate[1],\n",
    "                                recd_status,\n",
    "                                surname_canidate[2]\n",
    "                            )\n",
    "                else:\n",
    "                    if check_pos_tag(line, surname_canidate[2]):\n",
    "                        update_people_in_temp_telegram_object(\n",
    "                            temp_telegram_object,\n",
    "                            data_type,\n",
    "                            surname_canidate[0],\n",
    "                            '',\n",
    "                            lc_number,\n",
    "                            surname_canidate[1],\n",
    "                            recd_status,\n",
    "                            surname_canidate[2]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_sender_by_salutation(line, temp_telegram_object):\n",
    "    surname_canidate = \"\"\n",
    "    salutation_pattern = re.compile(r\"\\b(signed|obt servt|very respy)\\b\")\n",
    "    for match in re.finditer(salutation_pattern, line):\n",
    "        # is the salutation the last word in the telegram?\n",
    "        # to do: we don't account for this edge case\n",
    "        if last_word(match.end(), line):\n",
    "            return\n",
    "        \n",
    "        # we have a salutation, now we need search for titles and initials\n",
    "        # that may reside between the salutation and the sender's surname.\n",
    "        \n",
    "        # next_token is a tuple of (next_token, beginning index, end index)\n",
    "        canidate = next_token(line, match.end())\n",
    "        \n",
    "        # filter out possible titles\n",
    "        surname_canidate = find_titles(canidate[0], canidate[1], canidate[2], line)\n",
    "        \n",
    "        # filter out possible initials\n",
    "        surname_canidate = find_surname_candidate(surname_canidate[0], surname_canidate[1], surname_canidate[2], line)\n",
    "        if surname_canidate:\n",
    "            update_or_add_person(\n",
    "                line,\n",
    "                temp_telegram_object,\n",
    "                surname_canidate[1],\n",
    "                surname_canidate[2],\n",
    "                surname_canidate[0]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_known_people(line, temp_telegram_object):\n",
    "    split_telegram = line.split()\n",
    "    # do any of the tokens match known surnames?\n",
    "    found_surnames = [surname.lower() for surname in surnames if isinstance(surname, str) and surname.lower() in split_telegram]\n",
    "    # TO DO: account for multi-word last names\n",
    "    \n",
    "    for surname in set(found_surnames):\n",
    "        # now we can search for where in the telegram the match is found\n",
    "        for match in re.finditer(surname, line):\n",
    "            data_type = \"People\"\n",
    "            recd_status = False\n",
    "            lc_number = \"\"\n",
    "            \n",
    "            # if the match is not the first word, filter for location indicators\n",
    "            if match.start() != 0:\n",
    "                previous_token = line[0:match.start()].split()[-1]\n",
    "                if len(line[0:match.start()].split()) > 0:\n",
    "                    # can we also filter on grammar?\n",
    "                    if previous_token in ['camp', 'ft', 'fort', 'qrs', 'hdqrs']:\n",
    "                        return\n",
    "            \n",
    "            # this is no the last word, check the next token doesn't indicate place\n",
    "            if len(line[match.end():-1].split()) > 0:\n",
    "                post_token = line[match.end():-1].split()[0]\n",
    "                if post_token in ['city', 'road', 'river']:\n",
    "                    return\n",
    "            \n",
    "            surname_df = people[people.Surnames.str.lower() == surname]\n",
    "            \n",
    "            if len(surname_df) == 1:\n",
    "                lc_number = surname_df['LC_number'].values[0]\n",
    "            elif len(surname_df) > 1:\n",
    "                lc_number = 'multi-match'            \n",
    "            else:\n",
    "                lc_number = ''\n",
    "            if check_pos_tag(line, match.end()):\n",
    "                update_people_in_temp_telegram_object(\n",
    "                    temp_telegram_object,\n",
    "                    data_type,\n",
    "                    match.group(0).rstrip(),\n",
    "                    '',\n",
    "                    lc_number,\n",
    "                    match.start(),\n",
    "                    recd_status,\n",
    "                    match.end()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_people(line, temp_telegram_object):\n",
    "    find_known_people(line, temp_telegram_object)\n",
    "    find_person_by_title(line, temp_telegram_object)\n",
    "    \n",
    "    # deduplication is needed\n",
    "    people_dedup = []\n",
    "    for person in temp_telegram_object[\"People\"]:\n",
    "        if len(people_dedup) == 0:\n",
    "            people_dedup.append(person)\n",
    "        else:\n",
    "            captured = False\n",
    "            for new_person in people_dedup:\n",
    "                if new_person['start'] == person['start'] and new_person['end'] == person['end']:\n",
    "                    captured = True\n",
    "                    \n",
    "            if not captured:\n",
    "                people_dedup.append(person)\n",
    "    temp_telegram_object[\"People\"] = people_dedup            \n",
    "        \n",
    "    assign_sender_by_salutation(line, temp_telegram_object)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates to current entries should be handled in a different function\n",
    "def update_locations_in_temp_telegram_object(temp_telegram_object, data_type, text, controlled, lc_number, start, recd, end):\n",
    "    if not check_for_duplicates_by_location(\"Locations\", temp_telegram_object, start, end):\n",
    "        temp_telegram_object[\"Locations\"].append({\n",
    "            \"data_type\": data_type,\n",
    "            \"text\": text,\n",
    "            \"controlled\": controlled or '',\n",
    "            \"lc_number\": lc_number,\n",
    "            \"start\": start,\n",
    "            \"recd\": recd,\n",
    "            \"end\": end\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a token matching known state abbreviation/codes\n",
    "# this method needs work before being included\n",
    "def state_found(index, line):\n",
    "    split_telegram = line.split()\n",
    "    city_canidate = split_telegram[index - 1]\n",
    "    \n",
    "    pos_of_state_candidate = nltk.pos_tag(split_telegram)[index]\n",
    "    if city_canidate in simple_ordinals or pos_of_state_candidate[1] in ['IN', 'PRP', 'CC']:\n",
    "        return\n",
    "    \n",
    "    full_location = split_telegram[index - 1] + \" \" + split_telegram[index]\n",
    "    if full_location in all_titles:\n",
    "        return\n",
    "    \n",
    "    match = re.search(split_telegram[index], line)\n",
    "    return (match[0], match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_location_terms_found(index, line):\n",
    "    split_telegram = line.split()\n",
    "    signal = split_telegram[index]\n",
    "    previous_token = split_telegram[index - 1]\n",
    "    full_location = previous_token + \" \" + signal\n",
    "    \n",
    "    # don't include false positives like 'lt col'\n",
    "    if full_location in all_titles:\n",
    "        return\n",
    "    print(signal)\n",
    "    match = re.search(signal, line)\n",
    "    return (signal, match.start(), match.end())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# certain words like fort and creek signal the presence of a location token in the dataset\n",
    "def fort_signal_found(index, line):\n",
    "    split_telegram = line.split()\n",
    "    signal = split_telegram[index]\n",
    "    next_token = split_telegram[index + 1]\n",
    "    full_location = signal + \" \" + next_token\n",
    "    if full_location in all_titles:\n",
    "        return\n",
    "    print(\"full_location: \", full_location)\n",
    "    match = re.search(full_location, line)\n",
    "    if not match:\n",
    "        return None #an issue with \\n and not a continuation for the fort name\n",
    "    return (match[0], match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_signals = ['fort', 'ft']\n",
    "lower_location_terms = [location.lower() for location in location_terms]\n",
    "\n",
    "def find_locations(line, temp_telegram_object):\n",
    "    split_telegram = line.split()\n",
    "    data_type = \"location\"\n",
    "    controlled = \"\"\n",
    "    lc_number = \"\"\n",
    "    recd = \"\"\n",
    "    for index, word in enumerate(split_telegram):\n",
    "        if word in location_signals:\n",
    "            location_canidate = fort_signal_found(index, line)\n",
    "            if location_canidate is None:\n",
    "                return\n",
    "            update_locations_in_temp_telegram_object(\n",
    "                temp_telegram_object,\n",
    "                data_type,\n",
    "                location_canidate[0],\n",
    "                controlled,\n",
    "                lc_number,\n",
    "                location_canidate[1],\n",
    "                recd,\n",
    "                location_canidate[2]\n",
    "            )\n",
    "        elif word in lower_location_terms:\n",
    "            \n",
    "            location_canidate = lower_location_terms_found(index, line)\n",
    "            \n",
    "            location_df = locations[locations.Term.str.lower() == location_canidate[0]]\n",
    "            number_of_lc_matches = len(location_df)\n",
    "            \n",
    "            if number_of_lc_matches == 1:\n",
    "                lc_number = location_df.LC_number.values[0]\n",
    "            elif number_of_lc_matches > 1:\n",
    "                lc_number = 'multi-match'\n",
    "                \n",
    "            update_locations_in_temp_telegram_object(\n",
    "                temp_telegram_object,\n",
    "                data_type,\n",
    "                location_canidate[0],\n",
    "                controlled,\n",
    "                lc_number,\n",
    "                location_canidate[1],\n",
    "                recd,\n",
    "                location_canidate[2]\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for date and time tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate entries\n",
    "# updates to current entries should be handled in a different function\n",
    "def update_dates_in_temp_telegram_object(temp_telegram_object, data_type, text, controlled, start, recd, end):\n",
    "    if not check_for_duplicates_by_location(\"Dates\", temp_telegram_object, start, end):\n",
    "        temp_telegram_object[\"Dates\"].append({\n",
    "            \"data_type\": data_type,\n",
    "            \"text\": text,\n",
    "            \"controlled\": controlled or '',\n",
    "            \"lc_number\": '',\n",
    "            \"start\": start,\n",
    "            \"recd\": recd,\n",
    "            \"end\": end\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_times_in_temp_telegram_object(temp_telegram_object, data_type, text, controlled, start, recd, end):\n",
    "    if not check_for_duplicates_by_location(\"Times\", temp_telegram_object, start, end):\n",
    "        temp_telegram_object[\"Times\"].append({\n",
    "            \"data_type\": data_type,\n",
    "            \"text\": text,\n",
    "            \"controlled\": controlled or '',\n",
    "            \"lc_number\": '',\n",
    "            \"start\": start,\n",
    "            \"recd\": recd,\n",
    "            \"end\": end\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_recd_prefix(lines, start_index):\n",
    "    before_time_match = lines[0:start_index].rstrip()\n",
    "    match = re.search(r\"(recd|rcvd|revd)$\", before_time_match)\n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_morning_time(lines, temp_telegram_object):\n",
    "    for match in re.finditer(r\"\\b(?P<hour>[0-1]{0,1}[0-9]|[2]{0,1}[0-4])\\W{0,1}\\d{0,2}\\W?(a)\\.?(m)\\.?\", lines):\n",
    "        recd_time = detect_recd_prefix(lines, match.start())\n",
    "        data_type = \"Time\"\n",
    "        if recd_time:\n",
    "            data_type = \"Recd Time\"\n",
    "        update_times_in_temp_telegram_object(\n",
    "            temp_telegram_object,\n",
    "            data_type,\n",
    "            match.group(0).rstrip(),\n",
    "            '',\n",
    "            match.start(),\n",
    "            recd_time,\n",
    "            match.end()\n",
    "        )\n",
    "    return temp_telegram_object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see mssEC_01_043_p041_tel063.txt, matches \"12 mid\"\n",
    "def find_meridiem_time(lines, temp_telegram_object):\n",
    "    for match in re.finditer(strict_meridiem, lines):\n",
    "        recd_time = detect_recd_prefix(lines, match.start())\n",
    "        data_type = \"Time\"\n",
    "        if recd_time:\n",
    "            data_type = \"Recd Time\"\n",
    "        update_times_in_temp_telegram_object(\n",
    "            temp_telegram_object,\n",
    "            data_type,\n",
    "            match.group(0).rstrip(),\n",
    "            '',\n",
    "            match.start(),\n",
    "            recd_time,\n",
    "            match.end()\n",
    "        )\n",
    "    return temp_telegram_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evening_time(lines, temp_telegram_object):\n",
    "    evening_pattern = r\"\\b(?P<hour>[0-1]{0,1}[0-9]|[2]{0,1}[0-4])\\W{0,1}\\d{0,2}\\W?(p)\\.?(m)\\.?\"\n",
    "    for match in re.finditer(evening_pattern, lines):\n",
    "        recd_time = detect_recd_prefix(lines, match.start())\n",
    "        data_type = \"Time\"\n",
    "        if recd_time:\n",
    "            data_type = \"Recd Time\"\n",
    "        update_times_in_temp_telegram_object(\n",
    "            temp_telegram_object,\n",
    "            data_type,\n",
    "            match.group(0).rstrip(),\n",
    "            '',\n",
    "            match.start(),\n",
    "            recd_time,\n",
    "            match.end()\n",
    "        )\n",
    "    return temp_telegram_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_day(line, start_index, following_date_recd_status, temp_telegram_object):\n",
    "    before_time_match = line[0:start_index]\n",
    "    match = re.search(r\"\\d{1,2}\\w(st|d|th|nd|rd|[^AM]|[^PM])\", before_time_match)\n",
    "    # If the previous date is explicitly recieved,\n",
    "    # then the current date is a implicitly a sent date.\n",
    "    if match:\n",
    "        recd_status = False\n",
    "        # not a date, it is a time\n",
    "        if len(line[match.end():-1].split()) > 0:\n",
    "            post_token = line[match.end():-1].split()[0]\n",
    "            if post_token == 'am' or post_token == 'pm':\n",
    "                return\n",
    "        if following_date_recd_status:\n",
    "            data_type = \"Sent Date\"\n",
    "            update_dates_in_temp_telegram_object(\n",
    "                temp_telegram_object,\n",
    "                data_type,\n",
    "                match.group(0).rstrip(),\n",
    "                '',\n",
    "                match.start(),\n",
    "                recd_status,\n",
    "                match.end()\n",
    "            )\n",
    "        else:\n",
    "            data_type = \"Date\"\n",
    "            update_dates_in_temp_telegram_object(\n",
    "                temp_telegram_object,\n",
    "                data_type,\n",
    "                match.group(0).rstrip(),\n",
    "                '',\n",
    "                match.start(),\n",
    "                recd_status,\n",
    "                match.end()\n",
    "            )\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_date(line, temp_telegram_object):\n",
    "    for match in re.finditer(full_date_pattern, line):\n",
    "        data_type = \"Full Date\"\n",
    "        recd_status = detect_recd_prefix(line, match.start())\n",
    "        if recd_status:\n",
    "            data_type = \"Full Recd Date\"\n",
    "        if len(line[match.end():-1].split()) > 0:\n",
    "            post_token = line[match.end():-1].split()[0]\n",
    "            if post_token == 'am' or post_token == 'pm':\n",
    "                return\n",
    "        update_dates_in_temp_telegram_object(\n",
    "            temp_telegram_object,\n",
    "            data_type,\n",
    "            match.group(0).rstrip(),\n",
    "            '',\n",
    "            match.start(),\n",
    "            recd_status,\n",
    "            match.end()\n",
    "        )\n",
    "        prev_day(line, match.start(), recd_status, temp_telegram_object)\n",
    "         \n",
    "    return temp_telegram_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = [\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_019_p017_tel016/preprocessed_mssEC_03_019_p017_tel016.txt\",\n",
    "\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_031_p029_tel036/preprocessed_mssEC_03_031_p029_tel036.txt\",\n",
    "\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_041_p039_tel050/preprocessed_mssEC_03_041_p039_tel050.txt\",\n",
    "\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_042_p040_tel052/preprocessed_mssEC_03_042_p040_tel052.txt\",\n",
    "\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_043_p041_tel054/preprocessed_mssEC_03_043_p041_tel054.txt\",\n",
    "\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_050_p048_tel063/preprocessed_mssEC_03_050_p048_tel063.txt\",\n",
    "\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_054_p052_tel069/preprocessed_mssEC_03_054_p052_tel069.txt\",\n",
    "\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_063_p061_tel081/preprocessed_mssEC_03_063_p061_tel081.txt\",\n",
    "\"telegrams/clear_and_coded_telegrams/mssEC_03/mssEC_03_065_p063_tel083/preprocessed_mssEC_03_065_p063_tel083.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_05/mssEC_05_006_007_pp002_003_tel002/preprocessed_mssEC_05_006_007_pp002_003_tel002.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_05/mssEC_05_025_p021_tel016/preprocessed_mssEC_05_025_p021_tel016.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_05/mssEC_05_059_p055_tel058/preprocessed_mssEC_05_059_p055_tel058.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_05/mssEC_05_176_p172_tel185/preprocessed_mssEC_05_176_p172_tel185.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_05/mssEC_05_242_p238_tel262/preprocessed_mssEC_05_242_p238_tel262.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_08/mssEC_08_030_031_pp024_025_tel030/preprocessed_mssEC_08_030_031_pp024_025_tel030.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_08/mssEC_08_239_p233_tel275/preprocessed_mssEC_08_239_p233_tel275.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_08/mssEC_08_243_244_245_pp237_238_239_tel281/preprocessed_mssEC_08_243_244_245_pp237_238_239_tel281.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_08/mssEC_08_258_p252_tel290/preprocessed_mssEC_08_258_p252_tel290.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_08/mssEC_08_268_p262_tel301/preprocessed_mssEC_08_268_p262_tel301.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_11/mssEC_11_011_p005_tel009/preprocessed_mssEC_11_011_p005_tel009.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_11/mssEC_11_065_p059_tel095/preprocessed_mssEC_11_065_p059_tel095.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_11/mssEC_11_067_p061_tel100/preprocessed_mssEC_11_067_p061_tel100.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_11/mssEC_11_113_p107_tel188/preprocessed_mssEC_11_113_p107_tel188.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_11/mssEC_11_291_p285_tel459/preprocessed_mssEC_11_291_p285_tel459.txt\",\n",
    "\"telegrams/clear_telegrams/mssEC_11/mssEC_11_376_p370_tel593/preprocessed_mssEC_11_376_p370_tel593.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplication(key, temp_telegram_object):\n",
    "    deduplication_array = []\n",
    "    for element in temp_telegram_object[key]:\n",
    "        if len(deduplication_array) == 0:\n",
    "            deduplication_array.append(element)\n",
    "        else:\n",
    "            captured = False\n",
    "            for new_element in deduplication_array:\n",
    "                if new_element['start'] == element['start'] and new_element['end'] == element['end']:\n",
    "                    captured = True\n",
    "            if not captured:\n",
    "                deduplication_array.append(element)\n",
    "    temp_telegram_object[key] = deduplication_array\n",
    "    return temp_telegram_object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_metadata(telegram_number):\n",
    "    file_id = telegram_corpus.fileids()[telegram_number]\n",
    "    \n",
    "    # retrieve telegram text\n",
    "    telegram_text = telegram_corpus.raw(file_id)\n",
    "    \n",
    "    # create telegram object to track metadata extractions\n",
    "    telegram_object = create_temp_telegram_object()\n",
    "\n",
    "    print(telegram_text)\n",
    "    print(\"\")\n",
    "\n",
    "    find_morning_time(telegram_text, telegram_object)\n",
    "    telegram_object = deduplication(\"Times\", telegram_object)\n",
    "    \n",
    "    find_evening_time(telegram_text, telegram_object)\n",
    "    telegram_object = deduplication(\"Times\", telegram_object)\n",
    "    \n",
    "    find_date(telegram_text, telegram_object)\n",
    "    telegram_object = deduplication(\"Dates\", telegram_object)\n",
    "    \n",
    "    find_locations(telegram_text, telegram_object)\n",
    "    telegram_object = deduplication(\"Locations\", telegram_object)\n",
    "    \n",
    "    identify_people(telegram_text, telegram_object)\n",
    "    \n",
    "    # setup Pandas DataFrame\n",
    "    data_type = []\n",
    "    text = []\n",
    "    start = []\n",
    "    end = []\n",
    "    lc_number = []\n",
    "    \n",
    "    for entry in telegram_object['Times']:\n",
    "        data_type.append(entry['data_type'])\n",
    "        text.append(entry['text'])\n",
    "        start.append(entry['start'])\n",
    "        end.append(entry['end'])\n",
    "        lc_number.append(entry['lc_number'])\n",
    "    for entry in telegram_object['Dates']:\n",
    "        data_type.append(entry['data_type'])\n",
    "        text.append(entry['text'])\n",
    "        start.append(entry['start'])\n",
    "        end.append(entry['end'])\n",
    "        lc_number.append(entry['lc_number'])\n",
    "    for entry in telegram_object['People']:\n",
    "        data_type.append(entry['data_type'])\n",
    "        text.append(entry['text'])\n",
    "        start.append(entry['start'])\n",
    "        end.append(entry['end'])\n",
    "        lc_number.append(entry['lc_number'])\n",
    "    for entry in telegram_object['Locations']:\n",
    "        data_type.append(entry['data_type'])\n",
    "        text.append(entry['text'])\n",
    "        start.append(entry['start'])\n",
    "        end.append(entry['end'])\n",
    "        lc_number.append(entry['lc_number'])\n",
    "        \n",
    "    data = {'type': data_type, 'text': text, \"controlled\": '', \"lc_number\": lc_number, 'start': start, 'end': end}\n",
    "\n",
    "    df = pd.DataFrame(data=data)\n",
    "\n",
    "    return {'file_id': file_id, 'df': df}\n",
    "\n",
    "telegrams = range(len(telegram_corpus.fileids()))\n",
    "\n",
    "for telegram_number in telegrams:\n",
    "    found_metadata = find_metadata(telegram_number) # which is file_id\n",
    "    telegram_name = found_metadata['file_id'].split('/')[3]\n",
    "    ledger_name = telegram_name[0:12]\n",
    "    print(\"Telegram: \", telegram_name)\n",
    "    print(found_metadata['df'].sort_values(by=['start']))\n",
    "    found_metadata['df'].sort_values(by=['start']).to_csv('/Volumes/data_work/dcw_text_mining/example_metadata/' + telegram_name + '.csv')\n",
    "    \n",
    "    print(\"-------------------------------\")\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestStringMethods(unittest.TestCase):    \n",
    "    # --------------------------------\n",
    "    # METHOD find_evening_time()\n",
    "    # --------------------------------\n",
    "    def test_evening_time_without_puncuation(self):\n",
    "        example_line = \"louisville 9th 2 pm recd feb 9th 62 830 pm\"\n",
    "        self.assertEqual(len(find_evening_time(example_line, create_temp_telegram_object())[\"Times\"]), 2)\n",
    "\n",
    "    def test_evening_time_odd_puncuation(self):\n",
    "        example_line = \"mcclellans 11.15 pm june 26 62\"\n",
    "        self.assertEqual(len(find_evening_time(example_line, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "        \n",
    "    def test_evening_hour_single_number(self):\n",
    "        example_line = \"mcclellans 25 5 pm recd may 25 \"\n",
    "        self.assertEqual(len(find_evening_time(example_line, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "    \n",
    "    def test_evening_pm_lowercase(self):\n",
    "        example_line = \"mcclellans 25 5 pm  recd may 25 \"\n",
    "        self.assertEqual(len(find_evening_time(example_line, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "    \n",
    "    def test_evening_pm_mixcase_no_space(self):\n",
    "        example_line = \"ap chandler ny 3pm washn 12\"\n",
    "        self.assertEqual(len(find_evening_time(example_line, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "    \n",
    "    # --------------------------------\n",
    "    # METHOD find_meridiem_time()\n",
    "    # --------------------------------\n",
    "    def test_meridem_time(self):\n",
    "        example_line = \"louisville 12 mid 15th\"\n",
    "        self.assertEqual(len(find_meridiem_time(example_line, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "        example_line = \"louisville 12 m 15th\"\n",
    "        self.assertEqual(len(find_meridiem_time(example_line, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "    \n",
    "    def test_meridem_time_doesn_detect_impossible_times(self):\n",
    "        example_line = \"from louisville 19th  recd feb 20th 62\\n12 m\"\n",
    "        text_found = find_meridiem_time(example_line, create_temp_telegram_object())[\"Times\"][0][\"text\"]\n",
    "        self.assertEqual(text_found, \"12 m\")  \n",
    "    \n",
    "    # --------------------------------\n",
    "    # METHOD find_evening_time()\n",
    "    # --------------------------------\n",
    "    def test_morning_time(self):\n",
    "        example_line_1 = \"frederick 9 am 22d recd feb 22 62\"\n",
    "        example_line_2 = \"louisville 10 am 13th\"\n",
    "        example_line_3 = \"louisville 1035 am 13th\"\n",
    "        example_line_4 = \"louisville 1035am 13th\"\n",
    "        self.assertEqual(len(find_morning_time(example_line_1, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "        self.assertEqual(len(find_morning_time(example_line_2, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "        self.assertEqual(len(find_morning_time(example_line_3, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "        self.assertEqual(len(find_morning_time(example_line_4, create_temp_telegram_object())[\"Times\"]), 1)\n",
    "    \n",
    "    # --------------------------------\n",
    "    # METHOD find_date()\n",
    "    # --------------------------------\n",
    "    def test_date_finder(self):\n",
    "        example_line = 'nashville tenn march 21 62'\n",
    "        example_line2 = 'st louis 18 12 m recd Feb 18 62'\n",
    "        self.assertEqual(len(find_date(example_line, create_temp_telegram_object())[\"Dates\"]), 1)\n",
    "    \n",
    "    @unittest.skip(\"Pending Test\")\n",
    "    def test_no_time_included_in_date(self):\n",
    "        example_line = \"pattersons creek va 10th 6 pm recd feb 10th 10 pm\"\n",
    "    \n",
    "    # If two dates are supplied, but only one month and year, \n",
    "    # the first date is date sent, second is date recieved.\n",
    "    def test_date_finder_lone_sent_date(self):\n",
    "        example_line = 'st louis 18 12 m recd feb 18 62'\n",
    "        example_line_2 = 'pittsburg tenn 1st 3 pm recd may 2nd'\n",
    "        self.assertEqual(len(find_date(example_line, create_temp_telegram_object())[\"Dates\"]), 2)\n",
    "        self.assertEqual(len(find_date(example_line_2, create_temp_telegram_object())[\"Dates\"]), 2)\n",
    "    \n",
    "    def test_month_do_not_select_words_that_contain_month(self):\n",
    "        example_line = \"hello maynadier example\"\n",
    "        self.assertEqual(len(find_date(example_line, create_temp_telegram_object())[\"Dates\"]), 0)\n",
    "    \n",
    "    def test_find_two_dates_in_a_line(self):\n",
    "        example_line = \"louisville 19th recd feb 20 62\"\n",
    "        self.assertEqual(len(find_date(example_line, create_temp_telegram_object())[\"Dates\"]), 2)\n",
    "    \n",
    "    def test_find_date__do_not_include_beginning_time_as_a_date(self):\n",
    "        example_line = \"ft monroe 1st 1130 read july 1st 1140 am\"\n",
    "        found_date = find_date(example_line, create_temp_telegram_object())[\"Dates\"]\n",
    "        self.assertEqual(found_date[0]['text'], \"july 1st\")\n",
    "    \n",
    "    def test_find_date__with_month_abbreviation_mark(self):\n",
    "        example_line = \"a h caldwell no washn oct 8th 1866\"\n",
    "        found_date = find_date(example_line, create_temp_telegram_object())[\"Dates\"]\n",
    "        self.assertEqual(found_date[0]['text'], \"oct 8th 1866\")\n",
    "    \n",
    "    #     is the June here coded? mssEC_20_038_p032_tel062\n",
    "    def test_find_date__do_not_include_text_after_month(self):\n",
    "        example_line = \"june wedlock nancy for stephen yam\"\n",
    "        found_date = find_date(example_line, create_temp_telegram_object())[\"Dates\"]\n",
    "        self.assertEqual(found_date[0]['text'], \"june\")\n",
    "    \n",
    "    @unittest.skip(\"Pending Test\")\n",
    "    def test_do_not_include_extra_letters_from_next_line(self):\n",
    "        example_line = \"cairo march 27 1862  rcvd 27 march\\nh a wise navy dept quiet\"\n",
    "        found_date = find_date(example_line, create_temp_telegram_object())[\"Dates\"]\n",
    "        self.assertEqual(found_date[1]['text'], \"27 march\")\n",
    "        \n",
    "    # --------------------------------    \n",
    "    # METHOD detect_recd_prefix()\n",
    "    # --------------------------------\n",
    "    def test_detect_recd_prefix(self):\n",
    "        # TO DO: What if the Recvd date is before the sent date?\n",
    "        example_line_1 = \"louisville 9th 2 pm recd feb 9th 62 830 pm\"\n",
    "        example_line_2 = \"hampton march 31 62 rcvd apl 1\"\n",
    "        self.assertEqual(detect_recd_prefix(example_line_1, 25), True)\n",
    "        self.assertEqual(detect_recd_prefix(example_line_2, 25), True)\n",
    "    \n",
    "    # Case: mssEC_01_180_181_pp178_179_tel251.txt\n",
    "    def test_recd_prefix_should_only_apply_to_neighbor_date_objects(self):\n",
    "        example_line_3 = \"ft monroe 4th 120 pm recd jul 4 62 \\nnorfolk july fourth twelve thirty pm\"\n",
    "        self.assertEqual(detect_recd_prefix(example_line_3, 49), False)\n",
    "        \n",
    "    # --------------------------------    \n",
    "    # METHOD update_people()\n",
    "    # --------------------------------\n",
    "    def test_update_people_in_temp_telegram_object(self):\n",
    "        telegram_obj = create_temp_telegram_object()\n",
    "        update_people_in_temp_telegram_object(\n",
    "            telegram_obj,\n",
    "            \"People\",\n",
    "            \"halleck\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"817\",\n",
    "            False,\n",
    "            824\n",
    "        ) \n",
    "        update_people_in_temp_telegram_object(\n",
    "            telegram_obj,\n",
    "            \"People\",\n",
    "            \"halleck\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            817,\n",
    "            False,\n",
    "            824\n",
    "        ) \n",
    "        self.assertEqual(len(telegram_obj[\"People\"]), 1)\n",
    "        \n",
    "    def test_next_token(self):\n",
    "        line = \"hello what is the weather\"\n",
    "        next_token_data = next_token(line, 5)\n",
    "        self.assertEqual(next_token_data[0], \"what\")\n",
    "        self.assertEqual(next_token_data[1], 6)\n",
    "        self.assertEqual(next_token_data[2], 10)\n",
    "    \n",
    "    def test_find_surname_candidate(self):\n",
    "        canidate = \"r\"\n",
    "        beginning_index = 4\n",
    "        end_index = 5\n",
    "        line = \"gen r b marcy\"\n",
    "        surname_canidate = find_surname_candidate(canidate, beginning_index, end_index, line)\n",
    "        # (canidate, beginning_index, end_index)\n",
    "        self.assertEqual(surname_canidate[0], \"marcy\")\n",
    "        self.assertEqual(surname_canidate[1], 8)\n",
    "        self.assertEqual(surname_canidate[2], 13)\n",
    "        \n",
    "    def test_find_person_by_title(self):\n",
    "        line = \"gen r b marcy do not send the regular infantry\"\n",
    "        telegram_obj = create_temp_telegram_object()\n",
    "        person = find_person_by_title(line, telegram_obj)\n",
    "        self.assertEqual(len(telegram_obj[\"People\"]), 1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "\n",
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Corpus Stats\n",
    "\n",
    "This script provides some basic corpus stats for the Thomas T. Eckert collection telegrams. In addition to calculating these stats for the whole corpus, the script calculates the stats for each telegram ledger category.\n",
    "\n",
    "### References\n",
    "\n",
    "Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied text analysis with Python: Enabling language-aware data products with machine learning (First edition). O’Reilly Media, Inc.\n",
    "\n",
    "Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python (First edition). O’Reilly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Corpus\n",
    "This script uses NLTK's [CategorizedPlaintextCorpusReader](https://www.nltk.org/api/nltk.corpus.reader.html?highlight=categorizedplaintextcorpusreader#nltk.corpus.reader.CategorizedPlaintextCorpusReader) to create a corpus of telegrams. Based on folder structure, the script can consume:\n",
    "- all the telegrams ledgers\n",
    "- telegram ledgers that contain only telegrams in the clear (i.e., 'coded_telegrams')\n",
    "- telegram ledgers that contain only telegrams in code (i.e., 'clear_telegrams')\n",
    "- telegram ledgers that contain both telegram in the clear and telegrams in code (i.e., 'clear_and_coded_telegrams')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pattern = r'.*/preprocessed_.*.txt'\n",
    "category_pattern = r'.*?/(\\w+_telegrams)/'\n",
    "path_to_corpus = os.getenv('ECKERT_PAPERS_CORPUS_PATH')\n",
    "telegram_corpus = CategorizedPlaintextCorpusReader(\n",
    "    path_to_corpus,\n",
    "    doc_pattern,\n",
    "    cat_pattern=category_pattern\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all of the category labels, use the corpus class method `categories()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = telegram_corpus.categories()\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Generate General Stats\n",
    "After the corpus is setup, we can more easily describe the corpus in terms of general stats. For the whole corpus as well as each corpus category, this script calculates the number of files, words, non-stopwords, unique non-stopwords, lexical diversity, and average word use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_files = []\n",
    "number_of_words = []\n",
    "number_of_words_wo_stop_words = []\n",
    "number_of_unique_stopwords = []\n",
    "frequency_distribution_of_corpus_no_stop_words = []\n",
    "lexical_diversity = []\n",
    "avg_use_of_word = []\n",
    "\n",
    "# Add data for whole corpus\n",
    "number_of_files.append(len(telegram_corpus.fileids()))\n",
    "\n",
    "# create a list of all the words in a corpus\n",
    "whole_corpus_words = telegram_corpus.words()\n",
    "\n",
    "number_of_words.append(len(whole_corpus_words))\n",
    "\n",
    "# filter the corpus words list for english stopwords\n",
    "corpus_no_stopwords = [word for word in whole_corpus_words if word not in english_stop_words]\n",
    "number_of_words_no_stopwords = len(corpus_no_stopwords)\n",
    "number_of_words_wo_stop_words.append(number_of_words_no_stopwords)\n",
    "\n",
    "# set of unique words in the corpus minus stopwords\n",
    "set_corpus_no_stopwords = set(corpus_no_stopwords)\n",
    "number_of_unique_stopwords.append(len(set_corpus_no_stopwords))\n",
    "\n",
    "lexical_diversity.append(len(set_corpus_no_stopwords)/len(corpus_no_stopwords))\n",
    "avg_use_of_word.append(len(corpus_no_stopwords)/len(set_corpus_no_stopwords))\n",
    "\n",
    "for category in categories:\n",
    "    # number of files in the current category\n",
    "    number_of_files.append(len(telegram_corpus.fileids(categories=category)))\n",
    "    \n",
    "    # number of words in the corpus\n",
    "    corpus_words = telegram_corpus.words(categories=category)\n",
    "    number_of_words.append(len(corpus_words))\n",
    "    \n",
    "    # for effeciency comparisons, remove stopwords \n",
    "    corpus_no_stopwords = [word for word in corpus_words if word not in english_stop_words]\n",
    "    number_of_words_no_stopwords = len(corpus_no_stopwords)\n",
    "    number_of_words_wo_stop_words.append(number_of_words_no_stopwords)\n",
    "    \n",
    "    # how many of the unique words are there, excluding stopwords? \n",
    "    set_corpus_no_stopwords = set(corpus_no_stopwords)\n",
    "    number_of_unique_stopwords.append(len(set_corpus_no_stopwords))\n",
    "    \n",
    "    lexical_diversity.append(len(set_corpus_no_stopwords)/len(corpus_no_stopwords))\n",
    "    \n",
    "    avg_use_of_word.append(len(corpus_no_stopwords)/len(set_corpus_no_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ['whole_corpus', 'clear_and_coded_telegrams', 'clear_telegrams', 'coded_telegrams']\n",
    "category_data = {\n",
    "    'Number of Files': number_of_files,\n",
    "    'Number of Words': number_of_words,\n",
    "    'Number of Non-Stopwords': number_of_words_wo_stop_words,\n",
    "    'Number of Unique Non-Stopwords': number_of_unique_stopwords,\n",
    "    'Lexical Diversity': lexical_diversity,\n",
    "    'Average Word Use': avg_use_of_word\n",
    "}\n",
    "caption_text = \"Table 1. General Statistics for the Telegrams in The Thomas T. Eckert Papers\"\n",
    "category_comparison_data_frame = pd.DataFrame(data=category_data, index=indices).style.set_caption(caption_text).set_table_styles([\n",
    "    {'selector': 'caption', 'props': [('font-size','18px'),('color','black'), ('font-weight','bold')]},\n",
    "    {'td': 'caption', 'props': [('margin','1em')]}\n",
    "])\n",
    "category_comparison_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
